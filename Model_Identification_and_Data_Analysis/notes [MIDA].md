# Notes of Model Identification and Data Analysis

## Childhood Memories

- ${ae^{jw}=acos(w)+jasin(w)}$
- ${|z_1|^2=z_1\bar{z}_1}$
-  ${e^{j\omega}+e^{j(-\omega)}=2cos(\omega)}$



## Exercise 1

**Stationary Process**  
A process is stationary if and only if the poles of its transfer function are ${|z|\le 1}$ and if it is fed by a stationary noise signal 

**Compute the Variance of a signal ${y(z)=H(z)\eta (z)}$**

1. Transform the function in the domain of time

   ${\gamma(y(t))=E[\space (\space y(t)-E[\space (\space y(t)\space]\space)^2\space]}$

2. knowing that ${\gamma(y(t))=\gamma(y(t-1))}$ and that if ${y(t)=y(t-1)+\eta(t)}$ then ${\gamma(y(t))=\gamma(y(t))+\gamma(\eta(t))}$ if and only if ${y}$ and ${\eta}$ are independent, we can solve the last equation because usually the variance of ${\eta}$ is known.  
   Another important property of the variance is that ${\gamma(ay(t))=a^2 \gamma(y(t))}$ , given that ${a}$ is a constant.



**Variance and Spectra Properties**

If two signals ${\alpha}$ and ${\beta}$ are independent and ${\varphi=\alpha+\gamma}$:

-   ${\gamma_\varphi=\gamma_\alpha+\gamma_\beta}$
- ${\Gamma(\varphi(t))=\Gamma(\alpha(t))+\Gamma(\beta(t))}$



**All Pass Filter**

${y(t)=\frac{1+az}{1+\frac{1}{a}z}\varphi(t)=(\frac{1}{a}\frac{1+az}{1+\frac{1}{a}z})a\varphi(t)=a\varphi(t)}$



1. In case the transfer function is fed by a white noise with ${E[\eta(t)]=0}​$ we can simplify the expression in: 
   ${\gamma(y(t))=E[\space (\space y(t)\space)^2\space]}​$







## Exercise on Minimum Variance Controller

**Knowing the predictor ${y(t+2|t)}$, design the Minimum Variance Controller for the system, considering a reference signal ${y^0(t)}$.**    
$$
y(t)=\frac{C(z)}{A(z)}\eta(t)+B(z)
$$
We set ${\hat{y}(t+2|t)=y^0(t)}$.  
We are able to set it this way since ${B(z)}$ if and only if ${|z|<1}$.

Isolate ${u(t)}$. 

**Draw the block diagram of the control system ${X}$**  
the input is ${y^0(t)}$, the output is ${y(t)}$.  take the expression of ${y}$ that is provided to you by the text.





## Exercise on Prediction Error Estimation 

### For ${MA}$ Processes 

 **Suppose to have infinitely many data generated by the following system:**
$$
\delta: y(t)=2\eta (t) +4\eta(n-2)
$$
**where ${\eta(t)\sim WN(0.1)}​$ **

**For such data, consider the model classes:**
$$
M_1(a):y(t)=a_1y(t-1)+\xi_1(t)
$$

$$
M_2(a):y(t)=a_1y(t-2)+\xi_2(t)
$$

**where ${\xi_1(t)\sim WN(0,\lambda_1^2) \ and \ \xi_2(t)\sim 	wn(0,\lambda_2^2)}$**

**By using the prediction error minimization method, find the optimal estimates of parameters ${a_1}$ and ${a_2}$.**

in this case we have a ${MA(2)}$ for ${\delta}$.

This means that we can compute the covariances in the following way:  
Knowing that ${y(t)=c_0\eta(t)+c_1\eta(t-1)+...}$
$$
\gamma(0)=(c_0^2+c_1^2+...c_{n-1}^2)\gamma(\varepsilon(t))
$$

$$
\gamma(1)=(c_0c_1+c_1c_2+...+c_{n-2}c_{n-1})\gamma(\varepsilon(t))
$$

$$
\gamma(2)=(c_0c_2+c_1c_3+...+c_{n-3}c_{n-1})\gamma(\varepsilon(t))
$$

$$
\gamma(3)=(c_0c_3+c_1c_4+...+c_{n-4}c_{n-1})\gamma(\varepsilon(t))
$$

Let's consider only the first model:  
$$
M_1(a):y(t)=a_1y(t-1)+\xi_1(t)
$$
we want in in the form:
$$
y(t)=\frac{C(z)}{A(z)}\xi_1(t)
$$
and we obtain:
$$
C(z)=1 ; \ \ A(z)=1-a_1z^{-1}
$$
we can now write the predictor of ${y}$:
$$
\hat{y}(t|t-1)=\frac{C(z)-A(z)}{C(z)}y(t)=a_1z^{-1}y(t)
$$
And thus compute the prediction error ${\varepsilon}$:  
$$
\varepsilon(t)=y(t)-\hat{y}(t|t-1)=y(t)-a_1z^{-1}y(t)
$$
Now we can compute ${J}$, the performance index based on the prediction error:
$$
J=E[(\varepsilon(t))^2 ]=E[(y(t)-a_1y(t-1))^2]=E[y^2(t)]+a_1E[y^2(t-1)]-2a_1E[y(t)y(t-1)]
$$
We know that, being ${\eta\sim WN(0,1)}$, we have ${E[y^2(t)]=\gamma_y(0)=E[y(t-1)]}$

so:
$$
J=\gamma_y(0)+a_1^2\gamma_y(0)-2a_1\gamma_y(1)
$$
and we derive ${J}$ wrt ${a_1}$ and equal it to zero
$$
\frac{\delta J}{\delta a_1}=...=2(-\gamma_y(1)+a_1\gamma_y(0))=0
$$
Thus obtaining
$$
\hat{a}_1=0
$$
We repeat the process to find ${a_2}$ using the model ​ ${M_2}​$



### For ${AR}$ Processes  

**Knowing that ${\gamma_v(0)=10,\gamma_v(\pm 1)=0,\gamma_v(\pm2)=0,\gamma_v(\pm3)=3,\gamma_v(\tau)=0 \ \forall\tau:|\tau|\ge4  }$.  
Consider the model class**
$$
M:v(t)=av(t-1)+bv(t-2)+cv(t-3)+\eta(t)
$$
**with ${\eta \sim WN(0,\lambda^2)}$**

**By using the prediction error minimization method, find the optimal estimates of the parameters ${a}$, ${b}$ and ${c}$.**

The predictor is equal to the starting equation but for the ${\eta}​$ component:
$$
\hat{v}(t|t-1)=av(t-1)+bv(t-2)+cv(t-3)
$$
The prediction error ${\varepsilon}$ is always:
$$
\varepsilon(t)=v(t)-\hat{v}(t|t-1)
$$
Now let's compute ${J}$:
$$
J=E[(\varepsilon(t))^2]=
$$

$$
E[v^2(t)+a^2v^2(t-1)+b^2v^2(t-1)+c^2v^2(t-3)+
$$

$$
-2av(t)v(t-1)-2bv(t)v(t-2)-2cv(t)v(t-3)+
$$

$$
2abv(t-1)v(t-2)+2acv(t-1)v(t-3)+2bcv(t-2)v(t-3)]=
$$

$$
\gamma_v(0)+a^2\gamma_v(0)+b^2\gamma_v(0)+c^2\gamma_v(0)-2a\gamma_v(1)-2b\gamma_v(2)-2c\gamma_v(3)+2ab\gamma_v(1)+2ac\gamma_v(2)+2bc\gamma_v(1)
$$

$$
J=\gamma_v(0)( 1+a^2+b^2+c^2)-2c\gamma_v(3)
$$

To minimize ${J}$ we observe that we should impose ${\hat{a}=\hat{b}=0}$

and, as always, let's derive wrt ${c}$:
$$
\frac{\delta J}{\delta c}=2c\gamma_v(0)-2\gamma_v(3)\to \hat{c}=\frac{\gamma_v(3)}{\gamma_v(0)}=\frac{3}{10}
$$
